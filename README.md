# Proximal Action Replacement (PAR) for Behavior Cloning Actor-Critic in Offline Reinforcement Learning

![image1](imgs/d4rl_bar.png)

**Intro:** An easy-to-use plug-and-play training sample replacer. As training progresses, PAR progressively replaces low-value actions with high-value actions generated by a stable actor, reducing the impact of low-value data while broadening the action exploration space.

## Usage Guide

### Reproducing Toy Experiments

Running the toy experiment is straightforward, simply execute:

```bash
python toy_main.py
```

This script will automatically run the toy experiment and generate comparison plots, with results saved to [toy_comparison_grid.pdf](imgs/toy_comparison_grid.pdf).

### Reproducing Experimental Results with WandB (Offline Mode)

Use `main.py` for offline experiments (no network connection required, data saved locally and can be uploaded later).

**Required Parameters:**

- `--wandb_entity`: WandB username or organization name
- `--wandb_project`: WandB project name

**PAR-related Parameters:**

- `--algo`: Algorithm selection, options: `["TD3+BC", "EDP", "BCQ", "IQL", "SSAR"]`
- `--start_synthetic_epoch`: Epoch to start using synthetic data ($T_{start}$ in the paper)
- `--synthetic_percent_range`: Synthetic data percentage range ([$P_{min}$, $P_{max}$] in the paper), format as a tuple, e.g., `(0., 0.5)`
- `--LossMultiplier`: Loss multiplier (Î² in the paper), default value: 1.5

**Example Command:**

```bash
python main.py \
    --wandb_entity your_entity \
    --wandb_project your_project \
    --algo TD3+BC \
    --env_name walker2d-medium-replay-v2 \
    --start_synthetic_epoch 500 \
    --synthetic_percent_range "(0., 0.5)" \
    --LossMultiplier 1.1 
```

### Notice

The specific settings for the key hyperparameters ($T_{start}$, $P_{min}$, $P_{max}$, and $\beta$) are shown in Table 4 of the paper.
